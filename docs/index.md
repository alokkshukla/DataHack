---
layout: default
title: DataHack Summit 2017
---
# Sessions 
by [Alok K. Shukla](mailto:alokks2@illinois.edu?Subject=DataHack2017)

at [DataHack Summit 2017](https://www.analyticsvidhya.com/datahacksummit/)




#### Talk - [A Gentle Introduction to Bayesian Deep Learning](#talk)
#### Hack Session - [Visual Storytelling with D3](#session)


## A Gentle Introduction to Bayesian Deep Learning<a name="talk"></a>



This talk introduces the fundamental ideas behind the emergent field of Bayesian Deep Learning. 
Most of the material is borrowed from [NIPS Bayesian Deep Leraning Workshop](http://bayesiandeeplearning.org) proceedings. 
Many thanks to [Zoubin Ghahramani](http://mlg.eng.cam.ac.uk/zoubin/) and [Yarin Gal](http://www.cs.ox.ac.uk/people/yarin.gal/website/).

## Intended Audience

Someone with basic understanding fundamentals of Machine Learning and Neural Networks and The Bayes Rule.

## Outline

The talk is supposed to last about 45 minutes, with additional time for Q&A.

### Introduction

- Discussion about converging ideas
- How new and old amalgamate to bring something novel. 

### Bayes Rule and Bayesian Inference

- The famous Bayes Rule. 
- Discussion about priors, likelihood and posteriors. 
- The complete Bayesian Inference process. How posterior predictive models are used for estimation.  

### Neural Network - Revisited

- A simple neural network as function approximator. 
- Parameters and hyper-parameters of a Neural Network. 
- Discusion about point predictions and overfitting.

### Need of Bayesian Approach in Deep Learning

- How incorporating uncertainty is essential at times. 
- And what other benefits it might provide. 

### Bayesian Neural Network 

- Bringing priors to parameters in Neural Network. 
- Levels of unceratinty in a Neural Network.
- Implications. Discussion about how Dropout and Regularization can be expalined using Bayesain Reasoning.

### Bayesian Inference in Neural Networks

Discussion about various approches to posterior estimations - MCMC and Variation Inference.

### Bayesian Deep Learning in Practise

Introducing [Edward](https://edwardlib.org) - Tensors + Randomness

### Where are we?

Discussion about NIPS workshop. Leading researchers.

### Resources
- Bayesian Deep Learning Workshop NIPS 2016 <http://bayesiandeeplearning.org/2016/index.html> 
- Yarin Gal's PhD Thesis - Uncertainty in Deep Learning <http://mlg.eng.cam.ac.uk/yarin/blog_2248.html>


## Visual Storytelling with D3<a name="session"></a>
Huge thanks to [Mike Bostock](https://bost.ocks.org/mike/).

## Audience Pre-requisite 

- Must be comfortable with basics of HTML, CSS and JS
- Have a recent PC/ MAC with internet access and Chrome. 
- Sublime/ Atom or any preferred JS IDE
- A GitHub account for publishing 

## Outline

The jack session is designed to last about an hour.

### Introduction

- Why are we here? 
- Why should we care about D3?

### D3 Building Blocks

- Hello! DOM
- SVG and CSS
- JS Method Chaining
- Introducing D3 Selections

### Hello World! D3

- SVG Elements as building blocks
- Attributes
- Coming Alive - Intercations, Transitions, Delay

### Interactive Bar Chart

- Complete Bar Chart with Transitions 
- Concept of Enter, Exit

### D3 with Jupyter

An example of how to use D3 inside notebooks

### Publishing Blocks

- bl.ocks.org
- Block Builder

### Where Now?

- Vega, Vega-lite
- dc.js, Extensions

### Resources

Books, Blogs, Designers
